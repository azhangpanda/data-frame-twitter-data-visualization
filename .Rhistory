filtered.comps <- filter(current.comps.zips,current.comps.zips$zip == currentZip)
tmp.addr <- filtered.comps[sample(nrow(filtered.comps),1),]
x <- 1
while(x <= 15){
if(tmp.addr[1,]$zip == currentZip){
tmp.comps.zips <- data.frame(row.names=NULL,amount = numeric(),low=numeric(),high=numeric(),street=character(),zip=character(),latitude=numeric(),
longitude=numeric(), medianRealEstate=numeric(), minRealEstate=numeric(), maxRealEstate=numeric(),city = character(),state=character(),timestamp.zillow=as.Date(character()))
tmp.comps.zips <- getRealEstateValues(tmp.comps.zips,tmp.addr)
current.comps.zips <- rbind(current.comps.zips,tmp.comps.zips)
current.comps.zips = current.comps.zips[!duplicated(current.comps.zips$street),]
tmp.addr <- current.comps.zips[sample(nrow(tmp.comps.zips),1),]
} else {
tmp.addr <- current.comps.zips[sample(nrow(current.comps.zips),1),]
}
table <- as.data.frame(tally(zip,current.comps.zips),stringsAsFactors = FALSE)
currentZip <- table[table$Freq == min(table$Freq),]$Var1
x <- x + 1
}
master.zillow.zips <- rbind(master.zillow.zips,current.comps.zips)
master.zillow.zips <- unique(master.zillow.zips)
}
#amount dist
isMin <- TRUE
x <- 1
while(x <= 5){
if(isMin == TRUE){
tmp.addr <- current.comps.amount[current.comps.amount$amount==min(current.comps.amount$amount),]
isMin <-FALSE
}else{
tmp.addr <- current.comps.amount[current.comps.amount$amount==max(current.comps.amount$amount),]
isMin <-TRUE
}
tmp.comps.amount <- data.frame(row.names=NULL,amount = numeric(),low=numeric(),high=numeric(),street=character(),zip=character(),latitude=numeric(),
longitude=numeric(), medianRealEstate=numeric(), minRealEstate=numeric(), maxRealEstate=numeric(),city = character(),state=character(),timestamp.zillow=as.Date(character()))
tmp.comps.amount <- getRealEstateValues(tmp.comps.amount,tmp.addr)
current.comps.amount <- rbind(current.comps.amount,tmp.comps.amount)
current.comps.amount <- current.comps.amount[!duplicated(current.comps.amount$street),]
x <- x + 1
}
master.zillow.amount <- rbind(master.zillow.amount,current.comps.amount)
}
pathAmount <- paste("Macintosh HD/Users/aidaaiyizhang/Documents/academic/fall 14/data science","(",i,")",as.character(Sys.Date()),".xlsx",sep = "")
pathZip <- paste("Macintosh HD/Users/aidaaiyizhang/Documents/academic/fall 14/data science","(",i,")",as.character(Sys.Date()),".xlsx",sep = "")
write.csv(master.zillow.amount, pathAmount)
write.csv(master.zillow.zips, pathZip)
Sys.sleep(60*60*12)
}
for(j in 1:2){
for(i in 1:10){
current.comps.zips <- getRealEstateValues(current.comps.zips,addresses[i,])
current.comps.amount <- current.comps.zips
current.zips <- master.zips[master.zips$city == addresses[i,]$city,]
#zipcode dist
for(k in 1:nrow(current.zips)){
table <- as.data.frame(tally(zip,current.comps.zips),stringsAsFactors = FALSE)
currentZip <- table[table$Freq == min(table$Freq),]$Var1
filtered.comps <- filter(current.comps.zips,current.comps.zips$zip == currentZip)
tmp.addr <- filtered.comps[sample(nrow(filtered.comps),1),]
x <- 1
while(x <= 15){
if(tmp.addr[1,]$zip == currentZip){
tmp.comps.zips <- data.frame(row.names=NULL,amount = numeric(),low=numeric(),high=numeric(),street=character(),zip=character(),latitude=numeric(),
longitude=numeric(), medianRealEstate=numeric(), minRealEstate=numeric(), maxRealEstate=numeric(),city = character(),state=character(),timestamp.zillow=as.Date(character()))
tmp.comps.zips <- getRealEstateValues(tmp.comps.zips,tmp.addr)
current.comps.zips <- rbind(current.comps.zips,tmp.comps.zips)
current.comps.zips = current.comps.zips[!duplicated(current.comps.zips$street),]
tmp.addr <- current.comps.zips[sample(nrow(tmp.comps.zips),1),]
} else {
tmp.addr <- current.comps.zips[sample(nrow(current.comps.zips),1),]
}
table <- as.data.frame(tally(zip,current.comps.zips),stringsAsFactors = FALSE)
currentZip <- table[table$Freq == min(table$Freq),]$Var1
x <- x + 1
}
master.zillow.zips <- rbind(master.zillow.zips,current.comps.zips)
master.zillow.zips <- unique(master.zillow.zips)
}
#amount dist
isMin <- TRUE
x <- 1
while(x <= 5){
if(isMin == TRUE){
tmp.addr <- current.comps.amount[current.comps.amount$amount==min(current.comps.amount$amount),]
isMin <-FALSE
}else{
tmp.addr <- current.comps.amount[current.comps.amount$amount==max(current.comps.amount$amount),]
isMin <-TRUE
}
tmp.comps.amount <- data.frame(row.names=NULL,amount = numeric(),low=numeric(),high=numeric(),street=character(),zip=character(),latitude=numeric(),
longitude=numeric(), medianRealEstate=numeric(), minRealEstate=numeric(), maxRealEstate=numeric(),city = character(),state=character(),timestamp.zillow=as.Date(character()))
tmp.comps.amount <- getRealEstateValues(tmp.comps.amount,tmp.addr)
current.comps.amount <- rbind(current.comps.amount,tmp.comps.amount)
current.comps.amount <- current.comps.amount[!duplicated(current.comps.amount$street),]
x <- x + 1
}
master.zillow.amount <- rbind(master.zillow.amount,current.comps.amount)
}
pathAmount <- paste("Macintosh HD/Users/aidaaiyizhang/Documents/academic/fall 14/data science","(",i,")",as.character(Sys.Date()),".xlsx",sep = "")
pathZip <- paste("Macintosh HD/Users/aidaaiyizhang/Documents/academic/fall 14/data science","(",i,")",as.character(Sys.Date()),".xlsx",sep = "")
write.csv(master.zillow.amount, pathAmount)
write.csv(master.zillow.zips, pathZip)
Sys.sleep(60*60*12)
}
runApp("censusVis", display.mode = "showcase")
library(shiny)
runApp("censusVis", display.mode = "showcase")
runApp("App-1", display.mode = "showcase")
runApp("censusVis", display.mode = "showcase")
runApp("censusVis", display.mode = "showcase")
runApp("censusVis", display.mode = "showcase")
runApp("censusVis", display.mode = "showcase")
runApp("censusVis", display.mode = "showcase")
shiny::runApp('Documents/academic/fall 14/data science/App-1/census-app')
runApp("censusVis", display.mode = "showcase")
shiny::runApp('Documents/academic/fall 14/data science/App-1/census-app')
runApp("censusVis", display.mode = "showcase")
shiny::runApp('Documents/academic/fall 14/data science/App-1/census-app')
shiny::runApp('Documents/academic/fall 14/data science/App-1/census-app')
shiny::runApp('Documents/academic/fall 14/data science/App-1/census-app')
shiny::runApp('Documents/academic/fall 14/data science/App-1/census-app')
runApp("censusVis", display.mode = "showcase")
shiny::runApp('Documents/academic/fall 14/data science/App-1/census-app')
load("~/Desktop/twitter.RData")
require(mosaic)
test1=select(filteredTwitter, name, city, longitude, latitude, X.1)
test2=filter(test1, X.1=="11/13/2014")
test3=filter(test2, name="#AaronsNewVideo")
test3=filter(test2, name=="#AaronsNewVideo")
test4=group_by(test3, city)%>%summarise(Total=n())
test2=filter(test1, X.1=="11/13/2014", name=="#AaronsNewVideo")
View(test2)
test3=group_by(test2, city)%>%summarise(Total=n())
test1=select(filteredTwitter, name, city, longitude, latitude, X.1)
test2=filter(test1, X.1=="11/13/2014", name=="#AaronsNewVideo")
test3=group_by(test2, city)%>%summarise(Total=n())
test3=group_by(test2, city)%>%summarise(total=n())
test1=select(filteredTwitter, name, city, longitude, latitude, X.1)
test2=filter(test1, X.1=="11/13/2014", name=="#AaronsNewVideo")
test2=filter(test1, X.1=="11/13/2014")
View(test2)
test2=filter(test1, X.1=="11/13/2014", name=="Walmart")
View(test2)
test3=group_by(test2, city)%>%summarise(total=n())
View(test3)
test4=merge(x=test3, y=test1, by.x="city", by.y="city", all.x=TRUE)
test5=group_by(test4, name, city)%>%summarise(Tweets=n())
View(test5)
View(test4)
test1=select(filteredTwitter, name, city, longitude, latitude, X.1)
test2=filter(test1, X.1=="11/13/2014", name=="Walmart")
test3=group_by(test2, city)%>%summarise(total=n())
test4=merge(x=test3, y=test1, by.x="city", by.y="city", all.x=TRUE)
View(test4)
test2=filter(test1, X.1=="11/13/2014", name=="Walmart")
View(test2)
View(test2)
View(test3)
test3=group_by(test2, city)%>%summarise(tweets=n())
View(test3)
test4=merge(x=test3, y=test1, by.x="city", by.y="city", all.x=TRUE)
View(test4)
tset5=select(filteredTwitter, city, longitude, latitude)
tset4=select(filteredTwitter, city, longitude, latitude)
test5=merge(x=test3, y=test4, by.x="city", by.y="city", all.x=TRUE)
View(test5)
View(test3)
filteredTwitter <- read.csv("~/Desktop/filteredTwitter")
View(filteredTwitter)
test1=select(filteredTwitter, name, city, longitude, latitude, X.1)
test2=filter(test1, X.1=="11/13/2014", name=="Walmart")
test3=group_by(test2, city)%>%summarise(tweets=n())
tset4=select(filteredTwitter, city, longitude, latitude)
test5=merge(x=test3, y=test4, by.x="city", by.y="city", all.x=TRUE)
View(tset4)
test5=merge(x=test3, y=test4, by.x="city", by.y="city", all.x=TRUE)
test4=select(filteredTwitter, city, longitude, latitude)
test5=merge(x=test3, y=test4, by.x="city", by.y="city", all.x=TRUE)
View(test5)
View(test3)
View(test3)
View(test4)
View(test1)
View(filteredTwitter)
test5=merge(x=test3, y=test4, by.x="city", by.y="city", all.x=FALSE)
head(test5)
test5=merge(x=test3, y=test4, by="city")
View(test5)
View(test4)
test4=unique(select(filteredTwitter, city, longitude, latitude))
head(test4)
test5=merge(x=test3, y=test4, by.x="city", by.y="city", all.x=FALSE)
View(test5)
View(test5)
View(test5)
test6=tally(test5)
test6
test6=tally(test5)
test6=tally(select(test5, tweets))
test6
test6=sum(tweets)
test6=test5(sum(tweets))
test3=summarise(group_by(test2, city, tweets=n())
test3=summarise(group_by(test2, city, tweets=n()))
test3=summarise(group_by(test2, city, tweets=n()), N=sum(tweets))
test3=summarise(group_by(test2, city), tweets=n(), N=sum(tweets))
test3=group_by(test2, city)%>%summarise(N=n()), N=sum(tweets))
test3=group_by(test2, city)%>%summarise(N=n(), N=sum(tweets))
test3=group_by(test2, city)%>%summarise(N=n())
test1=select(filteredTwitter, name, city, longitude, latitude, X.1)
test2=filter(test1, X.1=="11/13/2014", name=="Walmart")
test3=group_by(test2, city)%>%summarise(tweets=n())
require(mosaic)
test1=select(filteredTwitter, name, city, longitude, latitude, X.1)
test2=filter(test1, X.1=="11/13/2014", name=="Walmart")
test3=group_by(test2, city)%>%summarise(tweets=n())
test4=unique(select(filteredTwitter, city, longitude, latitude))
View(test4)
test3%>%group_by(test2, city)%>%summarise(tweets=n())
test3%>%group_by(test2, city)%>%summarise(tweets=n())
test3%>%group_by(test2, city)%>%summarise(tweets=n(), N=sum(tweets))
test3=group_by(test2, city)%>%summarise(tweets=n(), N=sum(tweets))
group_by(test2, city)%>%summarise(tweets=n(), N=sum(tweets))
test3=group_by(test2, city)%>%summarise(N=n())
conflicts()
detach("package:plyr", unload=TRUE)
test1=select(filteredTwitter, name, city, longitude, latitude, X.1)
test2=filter(test1, X.1=="11/13/2014", name=="Walmart")
test3=group_by(test2, city)%>%summarise(N=n())
test3=group_by(test2, city)%>%summarise(tweets=n())
test3=group_by(test2, city)%>%summarise(tweets=n(), N=sum(tweets))
View(test3)
test4=unique(select(filteredTwitter, city, longitude, latitude))
test5=merge(x=test3, y=test4, by.x="city", by.y="city", all.x=FALSE)
View(test5)
test1=select(filteredTwitter, name, city, longitude, latitude, X.1)
test2=filter(test1, X.1=="11/13/2014", name=="Walmart")
test3=group_by(test2, city)%>%summarise(tweets=n())
test4=unique(select(filteredTwitter, city, longitude, latitude))
test5=merge(x=test3, y=test4, by.x="city", by.y="city", all.x=FALSE)
View(test5)
View(test5)
require(sp)
test3=select(test2, density, longitude, latitude)
test3=as.data.frame(test3)
coordinates(test3)=c("longitude", "latitude")
proj4string(test3)=CRS("+proj=longlat")
spplot(test3, z_col="tweets")
test1=select(filteredTwitter, name, city, longitude, latitude, X.1)
test2=filter(test1, X.1=="11/13/2014", name=="Walmart")
test3=group_by(test2, city)%>%summarise(tweets=n())
test4=unique(select(filteredTwitter, city, longitude, latitude))
test5=merge(x=test3, y=test4, by.x="city", by.y="city", all.x=FALSE)
require(sp)
test6=select(test5, density, longitude, latitude)
test6=as.data.frame(test6)
test6=select(test5, tweets, longitude, latitude)
test6=as.data.frame(test6)
coordinates(test6)=c("longitude", "latitude")
proj4string(test6)=CRS("+proj=longlat")
spplot(test6, z_col="tweets")
require(maptools)
boundary=readShapeSpatial("~/Documents/academic/fall 14/data science/map for twitter/gz_2010_us_040_00_500k//gz_2010_us_040_00_500k.shp", proj4string=CRS("+proj=longlat"))
require(rgdal)
proj.lcc = CRS("+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")
boundary.proj <- spTransform(boundary, proj.lcc)
boundary.proj <- spTransform(boundary, proj.lcc)
boundaryList =list("sp.polygons", boundary.proj)
spplot(test3, zcol="tweets", sp.layout=list(boundaryList))
spplot(test3, zcol="tweets", layout=list(boundaryList))
spplot(obj=test3, zcol="tweets", sp.layout=boundaryList, col.regions=brewer.pal(9,"PuBu"), cex=0.7)
require(RColorBrewer)
spplot(obj=test3, zcol="tweets", sp.layout=boundaryList, col.regions=brewer.pal(9,"PuBu"), cex=0.7)
```
spplot(obj=test5, zcol="tweets", sp.layout=boundaryList, col.regions=brewer.pal(9,"PuBu"), cex=0.7)
spplot(test6, zcol="tweets", layout=list(boundaryList))
spplot(test6, zcol="tweets", sp.layout=list(boundaryList))
require(RColorBrewer)
spplot(obj=test6, zcol="tweets", sp.layout=boundaryList, col.regions=brewer.pal(9,"PuBu"), cex=0.7)
spplot(test6, z_col="tweets", scales=list(draw = FALSE))
spplot(test6, z_col="tweets", scales=list(draw = TRUE))
View(filteredTwitter)
shiny::runApp('Documents/academic/fall 14/data science/map for twitter')
require(mosaic)
eqTwitter=read.csv("~/Documents/academic/fall 14/data science/map for twitter/data/eqTwitter")
zillow1204=read.csv("~/Documents/academic/fall 14/data science/map for twitter/data/zillowAmount(5)2014-12-03.csv")
test10=select(zillow1204, latitude, longitude, medianRealEstate, minRealEstate, maxRealEstate, city)
require(sp)
test11=select(test10, medianRealEstate, longitude, latitude)
test11=as.data.frame(test11)
coordinates(test11)=c("longitude", "latitude")
proj4string(test11)=CRS("+proj=longlat")
spplot(test11, z_col="medianRealEstate")
require(maptools)
boundary=readShapeSpatial("~/Documents/academic/fall 14/data science/map for twitter/data/gz_2010_us_040_00_500k//gz_2010_us_040_00_500k.shp", proj4string=CRS("+proj=longlat"))
require(maptools)
boundary=readShapeSpatial("~/Documents/academic/fall 14/data science/map for twitter/data/gz_2010_us_040_00_500k//gz_2010_us_040_00_500k.shp", proj4string=CRS("+proj=longlat"))
require(rgdal)
proj.lcc = CRS("+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")
boundary.proj <- spTransform(boundary, proj.lcc)
boundaryList =list("sp.polygons", boundary.proj)
spplot(test11, zcol="medianRealEstate", sp.layout=list(boundaryList))
require(RColorBrewer)
spplot(obj=test11, zcol="medianRealEstate", sp.layout=boundaryList, col.regions=brewer.pal(9,"PuBu"), cex=0.7)
test1=select(eqTwitter, name, city, longitude, latitude, X.1)
test2=filter(test1, X.1=="11/13/2014", name=="Walmart")
test3=group_by(test2, city)%>%summarise(tweets=n())
test4=unique(select(eqTwitter, city, longitude, latitude))
test5=merge(x=test3, y=test4, by.x="city", by.y="city", all.x=FALSE)
require(sp)
test6=select(test5, tweets, longitude, latitude)
test6=as.data.frame(test6)
coordinates(test6)=c("longitude", "latitude")
proj4string(test6)=CRS("+proj=longlat")
spplot(test6, z_col="tweets", scales=list(draw = TRUE))
require(maptools)
boundary=readShapeSpatial("~/Documents/academic/fall 14/data science/map for twitter/data/gz_2010_us_040_00_500k//gz_2010_us_040_00_500k.shp", proj4string=CRS("+proj=longlat"))
require(rgdal)
proj.lcc = CRS("+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")
boundary.proj <- spTransform(boundary, proj.lcc)
boundaryList =list("sp.polygons", boundary.proj)
spplot(test6, zcol="tweets", sp.layout=list(boundaryList))
require(RColorBrewer)
spplot(obj=test6, zcol="tweets", sp.layout=boundaryList, col.regions=brewer.pal(9,"PuBu"), cex=0.7)
shiny::runApp('Documents/academic/fall 14/data science/map for twitter')
shiny::runApp('Documents/academic/fall 14/data science/map for twitter')
require(mosaic)
eqTwitter=read.csv("~/Documents/academic/fall 14/data science/map for twitter/data/eqTwitter")
zillow1204=read.csv("~/Documents/academic/fall 14/data science/map for twitter/data/zillowAmount(5)2014-12-03.csv")
View(eqTwitter)
View(filteredTwitter)
zillow1204=read.csv("~/Documents/academic/fall 14/data science/map for twitter/data/jaccardHousing")
zillow1204=read.csv("~/Documents/academic/fall 14/data science/map for twitter/data/jaccardHousing")
view(jaccardHousing)
View(JaccardHousing)
eqTwitter=read.csv("~/Documents/academic/fall 14/data science/map for twitter/data/eqTwitter")
zillow1204=read.csv("~/Documents/academic/fall 14/data science/map for twitter/data/zillowAmount(5)2014-12-03.csv")
jaccardHousing=read.csv("~/Documents/academic/fall 14/data science/map for twitter/data/jaccardHousing")
View(jaccardHousing)
plot(jaccardHousing$jaccard, jaccardHousing$absMedDiff)
Albuquerque=filter(jaccardHousing, name1.1=="Albuquerque")
View(jaccardHousing)
head(Albuquerque)
Albuquerque=filter(jaccardHousing, name1=="Albuquerque")
plot(Albuquerque$jaccard, Albuquerque$absMedDiff)
Atlanta=filter(jaccardHousing, name1=="Atlanta")
plot(Atlanta$jaccard, Atlanta$absMedDiff)
xyplot( jaccard~ absMedDiff,data=Albuquerque, type=c("p", "l"))
xyplot( jaccard~ absMedDiff,data=Albuquerque)
xyplot( jaccard~ absMedDiff,data=Albuquerque, type=c( "l"))
xyplot( jaccard~ absMedDiff,data=Albuquerque, type=c("p", "r"))
xyplot( jaccard~ absMedDiff,data=Atlanta, type=c("p", "r"))
Atlanta=filter(jaccardHousing, name1=="Atlanta")
xyplot( jaccard~ absMedDiff,data=Atlanta, type=c("p", "r"))
plot(jaccardHousing$jaccard, jaccardHousing$absMedDiff)
xyplot( jaccard~ absMedDiff,data=jaccardHousing, type=c("p", "r"))
xyplot( jaccard~ absMedDiff,data=Austin, type=c("p", "r"))
Austin=filter(jaccardHousing, name1=="Austin")
xyplot( jaccard~ absMedDiff,data=Austin, type=c("p", "r"))
Albuquerque=filter(jaccardHousing, name1=="Albuquerque")
xyplot( jaccard~ absMedDiff,data=Albuquerque, type=c("p", "r"))
Atlanta=filter(jaccardHousing, name1=="Atlanta")
xyplot( jaccard~ absMedDiff,data=Atlanta, type=c("p", "r"))
Austin=filter(jaccardHousing, name1=="Austin")
Deep_South=filter(jaccardHousing, name1=="Atlanta", "Baton Rougue", "Brimingham", "Jackson")
Deep_South=filter(jaccardHousing, name1=="Atlanta", name1=="Baton Rougue", name1=="Brimingham", "Jackson")
Atlanta=filter(jaccardHousing, name1=="Atlanta")
BR=filter(jaccardHousing, name1=="Baton Rougue")
Brimingham=filter(jaccardHousing, name1=="Brimingham")
Jackson=filter(jaccardHousing, name1=="Jackson")
south=rbind(Atlant, BR, Brimingham, Jackson)
south=rbind(Atlanta, BR, Brimingham, Jackson)
head(south)
xyplot( jaccard~ absMedDiff,data=south, type=c("p", "r"))
Chicago=filter(jaccardHousing, name1=="Chicago")
Cincinnati=filter(jaccardHousing, name1=="Cincinnati")
Cleveland=filter(jaccardHousing, name1=="Cleveland")
Columbus=filter(jaccardHousing, name1=="Columbus")
lake=rbind(Chicago, Cincinnati, Cleveland, Columbus)
xyplot( jaccard~ absMedDiff,data=lake, type=c("p", "r"))
Atlanta=filter(jaccardHousing, name1=="Atlanta")
BR=filter(jaccardHousing, name1=="Baton Rougue")
Brimingham=filter(jaccardHousing, name1=="Brimingham")
Jackson=filter(jaccardHousing, name1=="Jackson")
south=rbind(Atlanta, BR, Brimingham, Jackson)
xyplot( jaccard~ absMedDiff,data=south, type=c("p", "r"))
Atlanta=filter(jaccardHousing, name1=="Atlanta")
BR=filter(jaccardHousing, name1=="Baton Rougue")
Brimingham=filter(jaccardHousing, name1=="Brimingham")
Jackson=filter(jaccardHousing, name1=="Jackson")
Alburquerque=filter(jaccardHousing, name1=="Alburquerque")
Austin=filter(jaccardHousing, name1=="Austin")
Baltimore=filter(jaccardHousing, name1=="Baltimore")
Dallas-Ft.worth=filter(jaccardHousing, name1=="Dallas-Ft.worth")
Greensboro=filter(jaccardHousing, name1=="Greensboro")
Houston=filter(jaccardHousing, name1=="Houston")
Jacksonville=filter(jaccardHousing, name1=="Jacksonville")
south=rbind(Atlanta, BR, Brimingham, Jackson, Alburquerque, Austin, Baltimore,Dallas-Ft.worth, Greensboro, Houston, Jacksonville)
south=rbind(Atlanta, BR, Brimingham, Jackson, Alburquerque, Austin, Baltimore, Dallas-Ft.worth, Greensboro, Houston, Jacksonville)
Dallas=filter(jaccardHousing, name1=="Dallas-Ft.worth")
south=rbind(Atlanta, BR, Brimingham, Jackson, Alburquerque, Austin, Baltimore, Dallas, Greensboro, Houston, Jacksonville)
xyplot( jaccard~ absMedDiff,data=south, type=c("p", "r"))
xyplot( jaccard~ absMedDiff,data=jaccardHousing, type=c("p", "r"))
xyplot( jaccard~ absMedDiff,data=south, type=c("p", "r"))
xyplot( jaccard~ absMedDiff,data=jaccardHousing, type=c("p", "r"))
xyplot( jaccard~ absMedDiff,data=lake, type=c("p", "r"))
xyplot( jaccard~ absMedDiff,data=south, type=c("p", "r"))
xyplot( jaccard~ absMedDiff,data=jaccardHousing, type=c("p", "r"), xlim=(0.05, 0.35))
xyplot( jaccard~ absMedDiff,data=jaccardHousing, type=c("p", "r"), xlim=c(0.05, 0.35))
xyplot( jaccard~ absMedDiff,data=jaccardHousing, type=c("p", "r"), ylim=c(0.05, 0.35))
xyplot( jaccard~ absMedDiff,data=jaccardHousing, type=c("p", "r"), ylim=c(0, 0.4))
xyplot( jaccard~ absMedDiff,data=jaccardHousing, type=c("p", "r"), ylim=c(0, 0.4), xlab="Jaccard Index", ylab="the difference between absolute median housing value")
xyplot( jaccard~ absMedDiff,data=jaccardHousing, type=c("p", "r"), ylim=c(0, 0.4), xlab="the difference between absolute median housing values", ylab="Jaccard Index")
xyplot( jaccard~ absMedDiff,data=lake, type=c("p", "r"))
xyplot( jaccard~ absMedDiff,data=south, type=c("p", "r"))
xyplot( jaccard~ absMedDiff,data=jaccardHousing, type=c("p", "r"), ylim=c(0, 0.4), xlab="the difference between absolute median housing values", ylab="Jaccard Index")
shiny::runApp('Documents/academic/fall 14/data science/df')
require(mosaic)
twitter=read.csv("~/Documents/academic/fall 14/data science/df/data/eqTwitter",header=TRUE, fileEncoding="latin1")
twitter=(select(twitter, name, city, X.1))
shiny::runApp('Documents/academic/fall 14/data science/df')
install.packages('devtools')
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='azhang', token='8BF5EB5223F40434DAA96426161CA917', secret='oE+IQRVIK/BEMTuFKGW8CuAqLAKf1ZQXgHxoanKt')
library(shinyapps)
setwd("~/Documents/academic/fall 14/data science/df")
shinyapps::deployApp('~/Documents/academic/fall 14/data science/df')
setwd("~/Documents/academic/fall 14/data science/df")
shinyapps::deployApp('p~/Documents/academic/fall 14/data science/df')
shinyapps::deployApp('~/Documents/academic/fall 14/data science/df')
runApp("df")
setwd("~/Documents/academic/fall 14/data science/df")
runApp("~/Documents/academic/fall 14/data science/df")
shinyapps::deployApp('~/Documents/academic/fall 14/data science/df')
normalizePath("~/Documents/academic/fall 14/data science/df/data/eqTwitter", winslash = "\\", mustWork = NA)
shinyapps::deployApp('~/Documents/academic/fall 14/data science/df')
deployApp(appName="df")
deployApp(appName='df')
deployApp('df')
deployApp(appName="df")
getwd()
deployApp('/Users/aidaaiyizhang/Documents/academic/fall 14/data science/df')
deployApp('/Users/aidaaiyizhang/Documents/academic/fall 14/data science/df')
runApp(df)
shiny::runApp()
deployApp('/Users/aidaaiyizhang/Documents/academic/fall 14/data science/df')
View(twitter)
View(twitter)
twitter=read.csv("~/Documents/academic/fall 14/data science/df/data/eqTwitter",header=TRUE, fileEncoding="latin1")
View(tset4)
View(twitter)
View(twitter)
install.packages('devtools')
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='azhang', token='8BF5EB5223F40434DAA96426161CA917', secret='oE+IQRVIK/BEMTuFKGW8CuAqLAKf1ZQXgHxoanKt')
library(shinyapps)
shinyapps::deployApp(appName="dataframe")
install.packages("BH")
shinyapps::deployApp(appName="dataframe")
deployApp(appName="dataframe")
shiny::runApp()
require(mosaic)
twitter=read.csv("eqTwitter.csv",header=TRUE,fileEncoding="latin1")
twitter=(select(twitter, name, city, X.1))
twitter=read.csv("data/eqTwitter.csv",header=TRUE,fileEncoding="latin1")
twitter=(select(twitter, name, city, X.1))
require(mosaic)
twitter=read.csv("data/eqTwitter.csv",header=TRUE,fileEncoding="latin1")
deploy(appName="dataframe")
library(shiny)
deployApp(appName="dataframe")
shiny::runApp()
setwd("~/Documents/academic/fall 14/data science/dataframe")
shiny::runApp()
shiny::runApp()
twitter=read.csv("~/Documents/academic/fall 14/data science/map for twitter/data/eqTwitter")
topic="Walmart"
day="11/17/2014"
test1=select(twitter, name, city, longitude, latitude, X.1)
test1$X.1=as.character(test1$X.1)
test2=mutate(test1, d=as.numeric(format(as.Date(test1$X.1,format="%m/%d/%y"),"%d")))
d=as.integer(format(as.Date(day,format="%m/%d/%y"),"%d"))
set=c(d:(d+7))
test2=subset(test2, d %in% set)
test3=filter(test2, name==topic)
test4=summarise(group_by(test3, city), tweets=n())
test5=unique(select(test2, city, longitude, latitude, X.1))
test6=merge(x=test4, y=test5, by.x="city", by.y="city", all.x=FALSE)
dotplot(tweets~city, data=test6,type=c("p", "h"), scales=list(rot=45))
dotplot(tweets~city, data=test6,type=c("p", "h"), scales=list(rot=45), ylab="number of tweets")
require(maps)
require(mapproj)
require(RColorBrewer)
map("state", proj="lambert", param=c(33,45), orientation=c(90,0,-100),mar=c(0,0,0,0) )
projData=mapproject(test6$longitude, test6$latitude, projection="lambert", param=c(33,45), orientation=c(90,0,-100))
head(projData)
projLong=projData$x
projLat=projData$y
text(projLong,projLat, test6$city, cex=0.7, pos=2)
require(RColorBrewer)
quantile(test6$tweets,seq(0,1,by=.25))
col=colorRampPalette(c(brewer.pal(9,"OrRd")))
col2=col(8)[as.numeric(cut(test6$tweets, breaks=quantile(test6$tweets, seq(0,1,length=8))))]
par(bg = "blue")
points(projLong,projLat,col=col2, pch=19, cex=0.8, bg="grey")
#   test7=filter(twitter, name==topic)
#   test7=filter(test7, X.1==day)
#   a=summarise(group_by(test7, name), N=n())
points(projLong,projLat,col=col2, cex=1.5, pch=19, ps=0.5)
require(SDMTools)
pnts=cbind(x=c(-0.26, -0.25, -0.25, -0.26), y=c(-0.9, -0.8, -0.9, -0.8))
a=sort(unique(col2))
legend.gradient(pnts, cols=a, c("100%", "0%"), title="quantile", ps=0.5)
